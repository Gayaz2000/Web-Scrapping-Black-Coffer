{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94d4bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "Lem = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6dce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_ingestion:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"Initialize with the file path.\"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def primary(self):\n",
    "        \"\"\"Load the primary data from the provided file path.\"\"\"\n",
    "        try:\n",
    "            data = pd.read_excel(self.file_path)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f'Error loading file: {e}')\n",
    "            return None\n",
    "\n",
    "    def fetch_data_from_url(self, url):\n",
    "        \"\"\"Fetch the data from the provided URL and return the article title and content.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = bs(response.text, 'html.parser')\n",
    "\n",
    "            article_title = soup.find('title').text if soup.find('title') else \"No Title\"\n",
    "            all_text_elements = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "\n",
    "            if all_text_elements:\n",
    "                all_text = all_text_elements.get_text(strip=True, separator=\"\\n\")\n",
    "                first_data = all_text.splitlines()\n",
    "            else:\n",
    "                first_data = []\n",
    "\n",
    "            return article_title, first_data\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL: {url}, Error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def secondary(self):\n",
    "        \"\"\"Process URLs from the dataset and save their content.\"\"\"\n",
    "        data = self.primary()\n",
    "        if data is None:\n",
    "            return None, None\n",
    "        \n",
    "        df = data.copy()\n",
    "        updated_list = []\n",
    "        no_matching_data = []\n",
    "        blank_link = []\n",
    "\n",
    "        for i, url in enumerate(df.get('URL', [])): \n",
    "            article_title, first_data = self.fetch_data_from_url(url)\n",
    "\n",
    "            if first_data is None:\n",
    "                print(f'No matching data found for URL: {url}')\n",
    "                blank_link.append(f\"blackassign00{i+1}: {url}\")\n",
    "                blank = {'URL_ID': f\"blackassign00{i+1}\", 'URL': url}\n",
    "                no_matching_data.append(blank)\n",
    "                continue\n",
    "\n",
    "            #update list with new entry\n",
    "            new_df = {\n",
    "                'URL_ID': df['URL_ID'][i] if 'URL_ID' in df else f\"assign00{i+1}\",\n",
    "                'URL': url,\n",
    "                'article_words': f'{article_title} - {first_data}'\n",
    "            }\n",
    "            updated_list.append(new_df)\n",
    "\n",
    "            # Save the article content to a text file\n",
    "            self.save_article_to_file(url, article_title, first_data)\n",
    "\n",
    "        return pd.DataFrame(updated_list), no_matching_data\n",
    "\n",
    "    def save_article_to_file(self, url, article_title, article_content):\n",
    "        \"\"\"Save the article content to a text file.\"\"\"\n",
    "        file_name = urllib.parse.quote_plus(url)  # URL-encoded file name\n",
    "        file_path = os.path.join(os.getcwd(), 'Text_files')\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(file_path)\n",
    "\n",
    "        with open(f'{file_path}/{file_name}.txt', 'w', encoding='utf-8') as file1:\n",
    "            file1.writelines(article_title + \"\\n\")\n",
    "            if not article_content:\n",
    "                file1.writelines(\"No data found\")\n",
    "            else:\n",
    "                file1.writelines('\\n'.join(article_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549c98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def StopWords_data(self, file_path=r'C:\\Users\\user\\Downloads\\StopWords-20241019T100716Z-001\\StopWords'):\n",
    "        StopWords_Auditor = open(f'{file_path}\\\\StopWords_Auditor.txt', 'r', encoding='ISO-8859-1')\n",
    "        StopWords_Currencies = open(f'{file_path}\\\\StopWords_Currencies.txt', 'r', encoding='ISO-8859-1')\n",
    "        StopWords_DateandNumbers = open(f'{file_path}\\\\StopWords_DatesandNumbers.txt', 'r', encoding='ISO-8859-1')\n",
    "        StopWords_Generic = open(f'{file_path}\\\\StopWords_Generic.txt', 'r', encoding='ISO-8859-1')\n",
    "        StopWords_GenericLong = open(f'{file_path}\\\\StopWords_GenericLong.txt', 'r', encoding='ISO-8859-1')\n",
    "        StopWords_Geographic = open(f'{file_path}\\\\StopWords_Geographic.txt', 'r', encoding='ISO-8859-1')\n",
    "        StopWords_Names = open(f'{file_path}\\\\StopWords_Names.txt', 'r', encoding='ISO-8859-1')\n",
    "\n",
    "        return StopWords_Auditor, StopWords_Currencies, StopWords_DateandNumbers, StopWords_Generic, StopWords_GenericLong, StopWords_Geographic, StopWords_Names\n",
    "    \n",
    "    def MasterDictionary_data(self, file_path=r'C:\\Users\\user\\Downloads\\MasterDictionary-20241019T100713Z-001\\MasterDictionary'):\n",
    "        # Negative Dictionary\n",
    "        file_neg = open(f'{file_path}\\\\negative-words.txt', 'r', encoding='ISO-8859-1')\n",
    "        file_neg.seek(0)\n",
    "        neg_split = file_neg.read().split()\n",
    "\n",
    "        # Positive Dictionary\n",
    "        file_pos = open(f'{file_path}\\\\positive-words.txt', 'r', encoding='ISO-8859-1')\n",
    "        file_pos.seek(0)\n",
    "        pos_split = file_pos.read().split()\n",
    "\n",
    "        return pos_split, neg_split\n",
    "    \n",
    "    def text_corpus(self, x):\n",
    "        StopWords_Auditor, StopWords_Currencies, StopWords_DateandNumbers, StopWords_Generic, StopWords_GenericLong, StopWords_Geographic, StopWords_Names = self.StopWords_data()\n",
    "\n",
    "        string_format = str(x).lower()\n",
    "        lowerwords = re.sub('[^a-zA-Z]+', ' ', string_format).strip()\n",
    "        token = word_tokenize(lowerwords)\n",
    "        token_word = [t for t in token if t not in (StopWords_Auditor, StopWords_Currencies, StopWords_DateandNumbers, StopWords_Generic, StopWords_GenericLong, StopWords_Geographic, StopWords_Names)]\n",
    "        lemmatized = [Lem.lemmatize(w) for w in token_word]\n",
    "        return lemmatized\n",
    "\n",
    "    def count_syllables(self, word):\n",
    "        vowels = 'aeiou'\n",
    "        count = 0\n",
    "        pre_char_was_vowel = False\n",
    "        exceptions = ['es', 'ed']\n",
    "\n",
    "        for exception in exceptions:\n",
    "            if word.endswith(exception):\n",
    "                return 0\n",
    "\n",
    "        for char in word.lower():\n",
    "            if char in vowels:\n",
    "                if not pre_char_was_vowel:\n",
    "                    count += 1\n",
    "                pre_char_was_vowel = True\n",
    "            else:\n",
    "                pre_char_was_vowel = False\n",
    "        return count\n",
    "\n",
    "    def calculate_complexity_percentage(self, words):\n",
    "        num_complex_words = sum(1 for word in words if self.count_syllables(word) >= 2)\n",
    "        total_words = len(words)\n",
    "        no_of_complex_words = num_complex_words\n",
    "        percentage_complex_words = (num_complex_words / total_words) * 100 if total_words > 0 else 0\n",
    "        return percentage_complex_words, no_of_complex_words\n",
    "\n",
    "    def count_syllable_per_word(self, words):\n",
    "        syllable_per_word = {word: self.count_syllables(word) for word in words}\n",
    "        return syllable_per_word\n",
    "\n",
    "    def personal_pronoun_count(self, words_list):\n",
    "        list_of_words = ['I', 'we', 'my', 'ours', 'us']\n",
    "        list_words_count = sum(1 for word in words_list if word in list_of_words)\n",
    "        return list_words_count\n",
    "\n",
    "    def avg_word_length(self, words):\n",
    "        count = sum(len(word) for word in words)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c68967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Col_Structure:\n",
    "    def col_structure_primary(self, data):\n",
    "        output_data = []\n",
    "        updated_list = []\n",
    "\n",
    "        analysis = Analysis()  # Instantiate once, use throughout the loop\n",
    "\n",
    "        for i, j, column in zip(data[\"URL_ID\"], data[\"URL\"], data[\"article_words\"]):\n",
    "            # Return tokenized words\n",
    "            preprocessed_word = analysis.text_corpus(column)\n",
    "\n",
    "            # Existing dict in the text file\n",
    "            positive_dict, negative_dict = analysis.MasterDictionary_data()\n",
    "\n",
    "            # Positive Score\n",
    "            positive_count = [ps_words for ps_words in preprocessed_word if ps_words in positive_dict]\n",
    "            positive_score = len(positive_count)\n",
    "\n",
    "            # Negative Score\n",
    "            negative_count = [ns_word for ns_word in preprocessed_word if ns_word in negative_dict]\n",
    "            negative_score = len(negative_count)\n",
    "\n",
    "            # Polarity Score\n",
    "            polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "            # Subjectivity Score\n",
    "            subjective_score = (positive_score + negative_score) / (len(preprocessed_word) + 0.000001)\n",
    "\n",
    "            # Average Sentence Length\n",
    "            total_sentence = len(nltk.tokenize.sent_tokenize(column))\n",
    "            avg_sentence_length = round(len(preprocessed_word) / total_sentence, 0)\n",
    "\n",
    "            # Percentage of Complex Words and Complex Words Count\n",
    "            percentage_of_ComplexWords, total_no_of_ComplexWords_count = analysis.calculate_complexity_percentage(preprocessed_word)\n",
    "\n",
    "            # FOG Index\n",
    "            FOG_index = 0.4 * (avg_sentence_length + percentage_of_ComplexWords)\n",
    "\n",
    "            # Avg No. of Words per Sentence\n",
    "            avg_no_of_words_per_sentence = round(len(column.split()) / total_sentence, 0)\n",
    "\n",
    "            # Word Count\n",
    "            word_count = len(preprocessed_word)\n",
    "\n",
    "            # Syllable per Word\n",
    "            syllable_per_word = analysis.count_syllable_per_word(preprocessed_word)\n",
    "\n",
    "            # Personal Pronouns\n",
    "            personal_pronouns = analysis.personal_pronoun_count(preprocessed_word)\n",
    "\n",
    "            # Average Word Length\n",
    "            word_length = analysis.avg_word_length(preprocessed_word)\n",
    "            avg_word_length = round(word_length / len(preprocessed_word), 0)\n",
    "\n",
    "            # Final dictionary to append to the list\n",
    "            final_dict = {\n",
    "                \"URL_ID\": i,\n",
    "                \"URL\": j,\n",
    "                \"article_word\": column,\n",
    "                \"Positive Score\": positive_score,\n",
    "                \"Negative Score\": negative_score,\n",
    "                \"Polarity Score\": polarity_score,\n",
    "                \"Subjectivity Score\": subjective_score,\n",
    "                \"Avg_Sentence_Length\": avg_sentence_length,\n",
    "                \"Percentage_of_ComplexWords\": percentage_of_ComplexWords,\n",
    "                \"FOG_index\": FOG_index,\n",
    "                \"Avg_No_of_Words_per_Sentence\": avg_no_of_words_per_sentence,\n",
    "                \"Complex_word_Count\": total_no_of_ComplexWords_count,\n",
    "                \"Word_Count\": word_count,\n",
    "                \"Syllable_Per_Word\": syllable_per_word,\n",
    "                \"Personal_Pronouns\": personal_pronouns,\n",
    "                \"Avg_Word_Length\": avg_word_length\n",
    "            }\n",
    "            updated_list.append(final_dict)\n",
    "\n",
    "        # Convert list of dictionaries to a DataFrame after the loop completes\n",
    "        df = pd.DataFrame(updated_list)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(r\"C:\\Users\\user\\Desktop\\Programming Languages\\My Projects\\Black_Coffer_Text_Analysis\\output_data.csv\", index=False)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506310f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = r'C:\\Users\\user\\Downloads\\input (1).xlsx'\n",
    "    obj     = Data_ingestion(file_path)\n",
    "    str_obj = Col_Structure()\n",
    "    obj1 = obj.primary()\n",
    "    total_data, blank_list = obj.secondary()\n",
    "    df = str_obj.col_structure_primary(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5bac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
